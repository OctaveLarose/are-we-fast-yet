---
title: "Section 5.5.2 Memory Usage for Program Representation"
output: html_document
---

```{r setup, include=FALSE, echo=FALSE}
source("scripts/libraries.R", chdir=TRUE)
source("scripts/data-processing2.R")

# will need some formatting
raw_data <- load_rebench_data_file("../../../memory-sec552.data")
raw_data <- factorize_result(raw_data)
raw_data <- subset(raw_data, suite == "progr-rep-mem")

data <- raw_data |>
  select(!c(suite, cores, extraargs, varvalue, machine)) |>
  filter(criterion == "MaxRSS", exe %in% c("PySOM-ast-int", "PySOM-bc-int",
                                           "TruffleSOM-ast-NativeCE-int", "TruffleSOM-bc-NativeCE-int" )) |>
  droplevels()

data$exe <- revalue(data$exe,
                    c("PySOM-bc-int"="PySOM BC",
                      "PySOM-ast-int"="PySOM AST",
                      "TruffleSOM-ast-NativeCE-int" = "TrSOM AST",
                      "TruffleSOM-bc-NativeCE-int" = "TrSOM BC"))

data <- data |>
  mutate(exe.bench = paste(exe, bench))
data$exe.bench <- factor(data$exe.bench)

med <- data |>
  group_by(bench, exe, inputsize, criterion, unit, exe.bench) |>
  summarise(median = median(value),
            .groups = "drop")
```



```{r maxrss-increase, include=FALSE, echo=FALSE}
# ggplot(med, aes(x=inputsize, y=median, group=exe.bench, color=exe.bench)) + geom_line()
```

#### Increase of MaxRSS for Each Copy of the SOM Unit Tests

This plot is not included in the paper, but shows that MaxRSS approximates well
the memory use of the program representation, since it is increasing with the
number of program copies.

```{r per-iter-increase, echo=FALSE}
med_diff <- med |>
  group_by(bench, exe, criterion, unit, exe.bench) |>
  mutate(diff = median - lag(median)) |>
  na.omit()
ggplot(med_diff, aes(x=inputsize, y=diff, group=exe.bench, color=exe.bench)) + geom_line()
```

#### Sec. 5.5.2 Numbers Used

**Warning**: The numbers here don't make sense. With the setup used in the artifact,
we run each benchmark once, but for 5 iterations. This means, the program representation
is drowned out by the program execution. These experiments would need to be
run separately For the paper, we ran them with 1 iteration and 5 invocations.


```{r stats, echo=FALSE}
stats <- med_diff |>
  group_by(bench, exe, criterion, unit, exe.bench) |>
  summarise(median.diff = median(diff),
            .groups = "drop") |>
  mutate(is.pysom = str_detect(exe, "PySOM"))

base <- stats |>
  filter(str_detect(exe, "BC")) |>
  mutate(base = median.diff) |>
  select(!c(exe.bench, median.diff, exe))

norm <- stats |>
  left_join(base, by = c("bench", "criterion", "unit", "is.pysom")) |>
  mutate(ratio = median.diff / base)

id <- function(x) { x }

tabular(bench*exe ~ (median.diff + ratio) * id, data=norm)
```
#### Lines of Code Statistics

```{r code-stats, echo=FALSE}
code_stats <- read.csv("../../../data/program-rep.cloc.csv")
code_stats$language <- factor(code_stats$language)
tabular(
  language ~ (files + blank + comment + code) * id,
  data=code_stats)
```
